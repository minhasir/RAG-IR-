This document explains the output files and metrics generated by the RAG evaluation system (main.py). The framework produces comprehensive results comparing RAG-enhanced responses against standard LLM outputs.

- HTML Report Features
Interactive summary of all key metrics

Side-by-side comparisons of RAG vs standard answers

Visualizations of performance differences

Example question analyses (first 3 questions shown by default)

- CSV/JSON Data Contents
Complete question-by-question results

Raw retrieval contexts and sources

Time measurements for each response

Similarity and overlap calculations

![alt text]([https://github.com/minhasir/RAG-IR-/blob/main/nq_processor.py]) ![alt text]([https://github.com/minhasir/RAG-IR-/blob/main/per_question_similarity.png]) ![alt text]([https://github.com/minhasir/RAG-IR-/blob/main/similarity_comparison.png]) ![alt text]([https://github.com/minhasir/RAG-IR-/blob/main/similarity_improvement_distribution.png])

