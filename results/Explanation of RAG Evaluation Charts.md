Based on the visualization function in our improved `main.py`, here's a detailed explanation of each chart's purpose and interpretation:

## 1. Response Time Comparison (response_time_comparison.png)

**Purpose**: Compares the average response time between the RAG system and standard LLM.  
**Interpretation**: This chart displays the average seconds required by each method to process identical questions. From the previous data, the RAG system is significantly faster than the standard LLM (4.56s vs 37.38s). This helps evaluate the performance advantages of RAG in practical applications.

## 2. Response Length Comparison (response_length_comparison.png)

**Purpose**: Compares the average character length of responses generated by RAG and standard LLM.  
**Interpretation**: This bar chart visually demonstrates the comprehensiveness of content generated by both methods. The data shows that RAG system outputs are notably shorter than standard LLM responses (175 characters vs 1910 characters), indicating that RAG is more concise but potentially lacks completeness.

## 3. Semantic Similarity Comparison (similarity_comparison.png)

**Purpose**: Evaluates the semantic similarity between responses generated by both methods and the reference answers.  
**Interpretation**: This chart displays semantic similarity scores calculated using embedding models. A score close to 1 indicates high similarity, while a score close to 0 indicates complete difference. The data shows that both methods have similar scores (0.558 vs 0.565), with RAG slightly lower.

## 4. Similarity Improvement Distribution (similarity_improvement_distribution.png)

**Purpose**: Shows the distribution of similarity improvements from RAG relative to standard LLM across all questions.  
**Interpretation**: This histogram displays the distribution of "RAG similarity minus standard LLM similarity." The center line represents 0 (no change), the right side shows positive values (RAG performs better), and the left side shows negative values (standard LLM performs better). This helps identify which types of questions RAG handles more effectively.

## 5. Per-Question Similarity Comparison (per_question_similarity.png)

**Purpose**: Provides a visual comparison of semantic similarity for each question across both methods.  
**Interpretation**: In this scatter plot, the X-axis represents standard LLM similarity scores, and the Y-axis represents RAG similarity scores. The diagonal line indicates equal performance between methods; points above the line show questions where RAG performs better, while points below the line indicate standard LLM superiority. This helps identify specific patterns or outliers.

## 6. Lexical Overlap Comparison (overlap_comparison.png)

**Purpose**: Compares the degree of vocabulary overlap between both methods and reference answers.  
**Interpretation**: Unlike semantic similarity, lexical overlap focuses on the proportion of identical key terms used. The data shows that RAG significantly outperforms standard LLM in vocabulary overlap (0.123 vs 0.071), indicating that RAG better captures key terminology and factual content.

Together, these charts provide a comprehensive perspective for understanding the advantages and limitations of RAG systems compared to standard LLMs. They particularly reveal RAG's advantages in speed and vocabulary precision, but disadvantages in content completeness, providing direction for further optimization.